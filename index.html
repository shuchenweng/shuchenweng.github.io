<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Shuchen Weng - BAAI</title>
  
  <meta name="author" content="Shuchen Weng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">

  <style>
    /* === æ ¸å¿ƒå¸ƒå±€è°ƒæ•´ === */
    body {
      background-color: #f0f2f5; /* åªæœ‰èƒŒæ™¯æ˜¯ç°è‰²çš„ */
      color: #333;
      font-family: 'Roboto', sans-serif;
      font-size: 17px; /* å­—å·åŠ å¤§ï¼Œé˜…è¯»æ›´èˆ’æœ */
      line-height: 1.6;
      margin: 0;
      padding: 40px 0; /* ä¸Šä¸‹ç•™ç™½ */
    }

    /* æ¨¡æ‹Ÿä¸€å¼ â€œç™½çº¸â€çš„æ•ˆæœï¼Œèšç„¦è§†çº¿ */
    .container {
      max-width: 900px; /* ã€å…³é”®æ”¹åŠ¨ã€‘ä»1200æ”¹å›900ï¼Œç´§å‡‘èšç„¦ */
      margin: 0 auto;
      background-color: #ffffff;
      padding: 50px 60px; /* å†…éƒ¨ç•™ç™½ */
      box-shadow: 0 4px 25px rgba(0,0,0,0.05); /* æ·¡æ·¡çš„æŠ•å½±ï¼Œå¢åŠ ç«‹ä½“æ„Ÿ */
      border-radius: 8px; /* è¾¹è§’åœ†æ¶¦ */
    }

    /* === é“¾æ¥ä¸æ–‡å­— === */
    a { color: #0066cc; text-decoration: none; transition: all 0.2s ease; }
    a:hover { color: #cc0000; background-color: #fff0f0; border-radius: 3px; }
    
    h1, h2, h3 { font-family: 'Lato', sans-serif; color: #111; }
    h1 { font-size: 2.4em; font-weight: 700; margin-bottom: 10px; letter-spacing: -0.5px; }
    
    /* æ ‡é¢˜å¸¦ä¸€ä¸ªå°è£…é¥°æ¡ï¼Œå¢åŠ ç²¾è‡´æ„Ÿ */
    h2 { 
      font-size: 1.5em; 
      margin-top: 50px; 
      margin-bottom: 25px;
      padding-bottom: 10px;
      border-bottom: 2px solid #f0f2f5;
      position: relative;
    }
    h2::after {
        content: '';
        display: block;
        width: 60px;
        height: 4px;
        background: #0066cc; /* è“è‰²çŸ­æ¨ªçº¿è£…é¥° */
        position: absolute;
        bottom: -3px;
        left: 0;
    }
    
    /* === é¡¶éƒ¨ä¸ªäººä¿¡æ¯ï¼ˆæ‹‰è¿‘è·ç¦»ï¼‰ === */
    .header-container {
      display: flex;
      align-items: flex-start; /* é¡¶éƒ¨å¯¹é½ */
      gap: 40px; /* ã€å…³é”®æ”¹åŠ¨ã€‘é—´è·ç¼©å°ï¼Œä¸å†å„åœ¨ä¸€è¾¹ */
    }
    .profile-info { flex: 1; } 
    .profile-img-container { flex-shrink: 0; } /* é˜²æ­¢å›¾ç‰‡è¢«å‹ç¼© */
    
    .profile-img { 
      width: 200px; /* å¤´åƒç¨å¾®è°ƒå°ä¸€ç‚¹ç‚¹ï¼Œæ›´åè°ƒ */
      height: 200px; 
      object-fit: cover; 
      border-radius: 50%; 
      box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }

    /* ç¤¾äº¤æŒ‰é’®ä¼˜åŒ– */
    .social-links { margin-top: 20px; }
    .social-links a {
      display: inline-block;
      margin-right: 12px;
      font-size: 0.95em;
      color: #444;
      background: #f7f9fa;
      padding: 5px 10px;
      border-radius: 5px;
      border: 1px solid #e1e4e8;
    }
    .social-links a:hover { background: #fff; border-color: #ccc; color: #000; }

    /* === è®ºæ–‡åˆ—è¡¨ä¼˜åŒ– === */
    .paper-item { 
      display: grid; 
      grid-template-columns: 220px 1fr; /* å·¦å›¾å®½åº¦å‡å°ï¼Œé€‚åº”æ•´ä½“å®½åº¦ */
      gap: 25px;
      margin-bottom: 35px;
      align-items: start;
    }

    .paper-img-box {
      width: 100%;
      height: 130px; /* é«˜åº¦å¾®è°ƒ */
      background-color: #eaeaea; /* ç°è‰²å ä½ */
      border-radius: 6px;
      overflow: hidden;
      border: 1px solid #eee;
    }
    .paper-img-box img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      transition: transform 0.3s;
    }
    .paper-img-box:hover img { transform: scale(1.05); }
    
    .paper-title { font-size: 1.15em; font-weight: 700; color: #000; margin-bottom: 6px; display: block; line-height: 1.3; }
    .paper-author { color: #555; margin-bottom: 6px; font-size: 1em; }
    .paper-venue { color: #cc0000; font-weight: bold; font-style: italic; font-size: 0.9em; }
    
    .paper-links { margin-top: 8px; font-size: 0.85em; }
    .paper-links a { margin-right: 12px; font-weight: 600; text-transform: uppercase; }

    .tag-new {
        background-color: #ffebeb;
        color: #d32f2f;
        font-size: 0.7em;
        padding: 2px 6px;
        border-radius: 4px;
        margin-left: 6px;
        vertical-align: middle;
        font-weight: 800;
        text-transform: uppercase;
    }

    /* ç§»åŠ¨ç«¯é€‚é… */
    @media (max-width: 768px) {
      .container { padding: 30px 20px; width: auto; margin: 0; box-shadow: none; }
      .header-container { flex-direction: column-reverse; text-align: center; align-items: center; gap: 20px; }
      .profile-img { width: 160px; height: 160px; }
      .paper-item { grid-template-columns: 1fr; gap: 15px; }
      .paper-img-box { height: auto; aspect-ratio: 16/9; }
    }
  </style>
</head>

<body>

  <div class="container">
  
    <div class="header-container">
      <div class="profile-info">
        <h1>Shuchen Weng (ç¿ä¹¦ç›)</h1>
        <p style="font-size: 1.1em; color: #555; margin-bottom: 15px;">
          Technical Staff at <b>Beijing Academy of Artificial Intelligence (BAAI)</b>
        </p>
        <p>
          I received my Ph.D. degree from Peking University in 2024, advised by Prof. <a href="#">Boxin Shi</a>. 
          Prior to that, I obtained my B.E. degree from Tianjin University. 
        </p>
        <p>
          My research interests lie in <b>Controllable Video Generation</b>, <b>Audio-Visual Learning</b>, and <b>Multimodal AI</b>.
        </p>
        <p style="font-family: monospace; font-size: 0.9em; background: #f4f4f4; display: inline-block; padding: 4px 8px; border-radius: 4px; margin-top: 5px;">
          Email: shuchenweng [at] pku.edu.cn
        </p>
        
        <div class="social-links">
          <a href="YOUR_GOOGLE_SCHOLAR_LINK">Google Scholar</a>
          <a href="YOUR_GITHUB_LINK">GitHub</a>
          <a href="YOUR_TWITTER_LINK">Twitter</a>
        </div>
      </div>
      
      <div class="profile-img-container">
        <img src="photo.jpg" alt="Shuchen Weng" class="profile-img">
      </div>
    </div>

    <h2>ğŸ”¥ News</h2>
    <ul style="padding-left: 20px; color: #444;">
      <li style="margin-bottom: 8px;"><b>[2025.06]</b> Three papers accepted by <b>NeurIPS 2025</b>!</li>
      <li style="margin-bottom: 8px;"><b>[2025.02]</b> One paper (VIRES) accepted by <b>CVPR 2025</b>.</li>
      <li><b>[2024.07]</b> Joined BAAI as a Technical Staff.</li>
    </ul>

    <h2>ğŸ“ Selected Publications</h2>
    <p style="color: #666; font-size: 0.9em; margin-bottom: 30px;">
      (* indicates equal contribution)
    </p>

    <div class="paper-item">
      <div class="paper-img-box">
          <img src="https://placehold.co/600x400/e0e0e0/999999?text=Paper+Image" alt="NeurIPS 2025">
      </div>
      <div class="paper-content">
        <span class="paper-title">Audio-Sync Video Generation with Multi-Stream Temporal Control <span class="tag-new">NeurIPS 2025</span></span>
        <span class="paper-author"><b>Shuchen Weng</b>*, Haojie Zheng*, Zheng Chang, Si Li, Boxin Shi, Xinlong Wang</span>
        <span class="paper-venue">NeurIPS 2025</span>
        <div class="paper-links">
          <a href="https://arxiv.org/pdf/2506.08003?">[PDF]</a>
          <a href="#">[Project Page]</a>
          <a href="#">[Code]</a>
        </div>
      </div>
    </div>

    <div class="paper-item">
      <div class="paper-img-box">
          <img src="https://placehold.co/600x400/e0e0e0/999999?text=PanoWan" alt="PanoWan">
      </div>
      <div class="paper-content">
        <span class="paper-title">PanoWan: Lifting Diffusion Video Generation Models to 360 with Latitude/Longitude-aware Mechanisms <span class="tag-new">NeurIPS 2025</span></span>
        <span class="paper-author">Yifei Xia*, <b>Shuchen Weng</b>*, Siqi Yang, Jingqi Liu, Chengxuan Zhu, Minggui Teng, Zijian Jia, Han Jiang, Boxin Shi</span>
        <span class="paper-venue">NeurIPS 2025</span>
        <div class="paper-links">
          <a href="https://arxiv.org/pdf/2505.22016">[PDF]</a>
        </div>
      </div>
    </div>

    <div class="paper-item">
      <div class="paper-img-box">
          <img src="https://placehold.co/600x400/e0e0e0/999999?text=VIRES" alt="VIRES">
      </div>
      <div class="paper-content">
        <span class="paper-title">VIRES: Video Instance Repainting with Sketch and Text Guidance</span>
        <span class="paper-author"><b>Shuchen Weng</b>*, Haojie Zheng*, Peixuan Zhan, Yuchen Hong, Han Jiang, Si Li, Boxin Shi</span>
        <span class="paper-venue">CVPR 2025</span>
        <div class="paper-links">
          <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Weng_VIRES_Video_Instance_Repainting_via_Sketch_and_Text_Guided_Generation_CVPR_2025_paper.pdf">[PDF]</a>
        </div>
      </div>
    </div>

    <div class="paper-item">
      <div class="paper-img-box">
          <img src="https://placehold.co/600x400/e0e0e0/999999?text=ICCV+2023" alt="ICCV 2023">
      </div>
      <div class="paper-content">
        <span class="paper-title">Affective Image Filter: Reflecting Emotions from Text to Images</span>
        <span class="paper-author"><b>Shuchen Weng</b>*, Peixuan Zhang*, Zheng Chang, Xinlong Wang, Si Li, Boxin Shi</span>
        <span class="paper-venue">ICCV 2023</span>
        <div class="paper-links">
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Weng_Affective_Image_Filter_Reflecting_Emotions_from_Text_to_Images_ICCV_2023_paper.pdf">[PDF]</a>
          <a href="https://github.com/zpx0922/AIFormer">[Code]</a>
        </div>
      </div>
    </div>

    <div class="paper-item">
      <div class="paper-img-box">
          <img src="https://placehold.co/600x400/e0e0e0/999999?text=ECCV+2022" alt="ECCV 2022">
      </div>
      <div class="paper-content">
        <span class="paper-title">CT<sup>2</sup>: Colorization Transformer via Color Tokens</span>
        <span class="paper-author"><b>Shuchen Weng</b>*, Jimeng Sun*, Yu Li, Si Li, Boxin Shi</span>
        <span class="paper-venue">ECCV 2022 (Oral)</span>
        <div class="paper-links">
          <a href="https://ci.idm.pku.edu.cn/Weng_ECCV22b.pdf">[PDF]</a>
          <a href="https://github.com/shuchenweng/CT2">[Code]</a>
        </div>
      </div>
    </div>

    <h2>ğŸ’¼ Experience</h2>
    <ul style="line-height: 1.8; color: #444;">
      <li><b>Beijing Academy of Artificial Intelligence (BAAI)</b> | Technical Staff | <i>Jul. 2024 - Present</i></li>
      <li><b>Beijing Academy of Artificial Intelligence (BAAI)</b> | Research Intern | <i>Jan. 2023 - Jun. 2024</i></li>
      <li><b>Peng Cheng Lab</b> | Research Intern | <i>Feb. 2019 - Aug. 2019</i></li>
      <li><b>Tencent AI Lab</b> | Research Intern | <i>Aug. 2018 - Nov. 2018</i></li>
    </ul>

    <h2>ğŸ† Selected Awards</h2>
    <ul style="line-height: 1.8; color: #444;">
      <li>Huawei TopMinds Program (åä¸ºå¤©æ‰å°‘å¹´), 2024</li>
      <li>Alibaba Taotian Group T-star Program (é˜¿é‡Œæ·˜å¤©T-Star), 2024</li>
      <li>National Scholarship for Graduate Student, Peking University (Top 2%), 2023</li>
    </ul>

    <div style="margin-top: 60px; color: #bbb; font-size: 0.8em; text-align: center;">
      <p>Last updated: Jan. 2026</p>
    </div>

  </div> </body>
</html>
