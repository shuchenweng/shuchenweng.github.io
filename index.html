<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Shuchen Weng - BAAI</title>
  
  <meta name="author" content="Shuchen Weng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">

<style>
    /* === å…¨å±€é‡ç½®ä¸èƒŒæ™¯è‰² === */
    body {
      background-color: #f4f6f8; /* ææ·¡çš„ç°è“è‰²èƒŒæ™¯ï¼ŒæŠ¤çœ¼ä¸”é«˜çº§ */
      color: #333;
      font-family: 'Roboto', sans-serif;
      font-size: 16px;
      line-height: 1.6;
      margin: 0;
      padding: 0;
    }

    /* å®¹å™¨å±…ä¸­ */
    .container {
      max-width: 1100px;
      margin: 40px auto;
      padding: 0 20px;
    }

    a { color: #0066cc; text-decoration: none; transition: all 0.2s ease; }
    a:hover { color: #cc0000; }
    
    h1, h2, h3 { font-family: 'Lato', sans-serif; color: #111; }
    h1 { font-size: 2.2em; font-weight: 700; margin-bottom: 10px; }
    h2 { 
      font-size: 1.5em; 
      color: #2c3e50;
      margin-top: 50px; 
      margin-bottom: 25px;
      padding-left: 10px;
      border-left: 5px solid #0066cc; /* å·¦ä¾§è“è‰²ç«–æ¡è£…é¥° */
    }
    
    /* === å¡ç‰‡å¼è®¾è®¡æ ¸å¿ƒ === */
    .card {
      background: #ffffff;
      padding: 30px;
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(0,0,0,0.05); /* æŸ”å’Œçš„é˜´å½± */
      margin-bottom: 30px;
      transition: transform 0.2s ease;
    }
    /* é¼ æ ‡æ”¾ä¸Šå»å¾®å¾®ä¸Šæµ® */
    .paper-item:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 25px rgba(0,0,0,0.08);
    }

    /* === é¡¶éƒ¨ä¸ªäººä¿¡æ¯ === */
    .header-container {
      display: flex;
      align-items: center;
      gap: 50px;
    }
    .profile-info { flex: 7; }
    .profile-img-container { flex: 3; text-align: center; }
    .profile-img { 
      width: 220px; 
      height: 220px; 
      object-fit: cover; 
      border-radius: 50%; 
      border: 5px solid #fff; /* ç…§ç‰‡åŠ ç™½è¾¹ */
      box-shadow: 0 8px 20px rgba(0,0,0,0.15);
    }
    
    /* ç¤¾äº¤æŒ‰é’®ç¾åŒ– */
    .social-links { margin-top: 20px; }
    .social-btn {
      display: inline-block;
      background: #eff3f6;
      color: #333;
      padding: 6px 12px;
      border-radius: 6px;
      font-size: 0.9em;
      font-weight: 500;
      margin-right: 10px;
      border: 1px solid #e1e4e8;
    }
    .social-btn:hover { background: #e1e4e8; text-decoration: none; color: #000; }

    /* === è®ºæ–‡åˆ—è¡¨ === */
    .paper-item { 
      background: #ffffff;
      padding: 25px;
      border-radius: 10px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.03);
      display: grid; 
      grid-template-columns: 240px 1fr; 
      gap: 30px;
      margin-bottom: 25px;
      align-items: start;
    }

    .paper-img-box {
      width: 100%;
      height: 150px;
      background-color: #e0e0e0; /* è¿™é‡Œçš„ç°è‰²ä»¥åä¼šè¢«ä½ çš„å›¾ç‰‡ç›–ä½ */
      border-radius: 6px;
      overflow: hidden;
    }
    .paper-img-box img {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    
    .paper-content { display: flex; flex-direction: column; }
    .paper-title { font-size: 1.15em; font-weight: 700; color: #222; margin-bottom: 6px; line-height: 1.4; }
    .paper-author { color: #555; margin-bottom: 8px; font-size: 1em; }
    .paper-venue { color: #cc0000; font-weight: bold; font-style: italic; }

    /* è®ºæ–‡é“¾æ¥çš„å°æŒ‰é’®æ ·å¼ */
    .btn-link {
        display: inline-block;
        font-size: 0.8em;
        padding: 2px 8px;
        border: 1px solid #0066cc;
        border-radius: 4px;
        color: #0066cc;
        margin-right: 8px;
        margin-top: 8px;
        font-weight: 500;
    }
    .btn-link:hover {
        background-color: #0066cc;
        color: #fff;
        text-decoration: none;
    }

    /* æ ‡ç­¾ */
    .tag-new {
        background-color: #ffebeb;
        color: #d32f2f;
        font-size: 0.7em;
        padding: 2px 6px;
        border-radius: 4px;
        margin-left: 8px;
        vertical-align: middle;
        font-weight: 800;
        text-transform: uppercase;
    }

    /* ç§»åŠ¨ç«¯é€‚é… */
    @media (max-width: 768px) {
      .header-container { flex-direction: column-reverse; text-align: center; gap: 20px; }
      .profile-img { width: 180px; height: 180px; }
      .paper-item { grid-template-columns: 1fr; gap: 15px; padding: 20px;}
      .paper-img-box { height: auto; aspect-ratio: 16/9; }
    }
  </style>
</head>

<body>

  <div class="header-container">
    <div class="profile-info">
      <h1>Shuchen Weng (ç¿ä¹¦ç›)</h1>
      <p style="font-size: 1.2em; color: #555; margin-bottom: 20px;">
        Technical Staff at <b>Beijing Academy of Artificial Intelligence (BAAI)</b>
      </p>
      <p style="margin-bottom: 20px;">
        I am currently a Technical Staff at <a href="https://www.baai.ac.cn/en">BAAI</a>. 
        I received my Ph.D. degree from Peking University in 2024, advised by Prof. <a href="#">Boxin Shi</a>. 
        Prior to that, I obtained my B.E. degree from Tianjin University. 
        <br><br>
        My research interests lie in <b>Controllable Video Generation</b>, <b>Audio-Visual Learning</b>, and <b>Multimodal AI</b>.
      </p>
      <p style="font-family: monospace; background: #f9f9f9; display: inline-block; padding: 5px 10px; border-radius: 5px;">
        Email: shuchenweng [at] pku.edu.cn
      </p>
      
      <div style="margin-top: 25px;">
        <a href="YOUR_GOOGLE_SCHOLAR_LINK" style="margin-right: 20px;">[Google Scholar]</a>
        <a href="YOUR_GITHUB_LINK" style="margin-right: 20px;">[GitHub]</a>
        <a href="YOUR_TWITTER_LINK">[Twitter]</a>
      </div>
    </div>
    
    <div class="profile-img-container">
      <img src="photo.jpg" alt="Shuchen Weng" class="profile-img">
    </div>
  </div>

  <h2>ğŸ”¥ News</h2>
  <ul style="line-height: 2;">
    <li><b>[2025.06]</b> Three papers accepted by <b>NeurIPS 2025</b>!</li>
    <li><b>[2025.02]</b> One paper (VIRES) accepted by <b>CVPR 2025</b>.</li>
    <li><b>[2024.07]</b> Joined BAAI as a Technical Staff.</li>
  </ul>

  <h2>ğŸ“ Selected Publications</h2>
  <p style="color: #666; margin-bottom: 40px;">
    * indicates equal contribution. Full list available on Google Scholar.
  </p>

  <div class="paper-item">
    <div class="paper-img-box">
        <img src="https://placehold.co/600x400/e0e0e0/999999?text=Paper+Image" alt="NeurIPS 2025">
    </div>
    <div class="paper-content">
      <span class="paper-title">Audio-Sync Video Generation with Multi-Stream Temporal Control <span class="tag-new">NeurIPS 2025</span></span>
      <span class="paper-author"><b>Shuchen Weng</b>*, Haojie Zheng*, Zheng Chang, Si Li, Boxin Shi, Xinlong Wang</span>
      <span class="paper-venue">NeurIPS 2025</span>
      <div class="paper-links">
        <a href="https://arxiv.org/pdf/2506.08003?">[PDF]</a>
        <a href="#">[Project Page]</a>
        <a href="#">[Code]</a>
      </div>
    </div>
  </div>

  <div class="paper-item">
    <div class="paper-img-box">
        <img src="https://placehold.co/600x400/e0e0e0/999999?text=PanoWan" alt="PanoWan">
    </div>
    <div class="paper-content">
      <span class="paper-title">PanoWan: Lifting Diffusion Video Generation Models to 360 with Latitude/Longitude-aware Mechanisms <span class="tag-new">NeurIPS 2025</span></span>
      <span class="paper-author">Yifei Xia*, <b>Shuchen Weng</b>*, Siqi Yang, Jingqi Liu, Chengxuan Zhu, Minggui Teng, Zijian Jia, Han Jiang, Boxin Shi</span>
      <span class="paper-venue">NeurIPS 2025</span>
      <div class="paper-links">
        <a href="https://arxiv.org/pdf/2505.22016">[PDF]</a>
      </div>
    </div>
  </div>

  <div class="paper-item">
    <div class="paper-img-box">
        <img src="https://placehold.co/600x400/e0e0e0/999999?text=VIRES" alt="VIRES">
    </div>
    <div class="paper-content">
      <span class="paper-title">VIRES: Video Instance Repainting with Sketch and Text Guidance</span>
      <span class="paper-author"><b>Shuchen Weng</b>*, Haojie Zheng*, Peixuan Zhan, Yuchen Hong, Han Jiang, Si Li, Boxin Shi</span>
      <span class="paper-venue">CVPR 2025</span>
      <div class="paper-links">
        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Weng_VIRES_Video_Instance_Repainting_via_Sketch_and_Text_Guided_Generation_CVPR_2025_paper.pdf">[PDF]</a>
      </div>
    </div>
  </div>

  <div class="paper-item">
    <div class="paper-img-box">
        <img src="https://placehold.co/600x400/e0e0e0/999999?text=ICCV+2023" alt="ICCV 2023">
    </div>
    <div class="paper-content">
      <span class="paper-title">Affective Image Filter: Reflecting Emotions from Text to Images</span>
      <span class="paper-author"><b>Shuchen Weng</b>*, Peixuan Zhang*, Zheng Chang, Xinlong Wang, Si Li, Boxin Shi</span>
      <span class="paper-venue">ICCV 2023</span>
      <div class="paper-links">
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Weng_Affective_Image_Filter_Reflecting_Emotions_from_Text_to_Images_ICCV_2023_paper.pdf">[PDF]</a>
        <a href="https://github.com/zpx0922/AIFormer">[Code]</a>
      </div>
    </div>
  </div>

  <div class="paper-item">
    <div class="paper-img-box">
        <img src="https://placehold.co/600x400/e0e0e0/999999?text=ECCV+2022" alt="ECCV 2022">
    </div>
    <div class="paper-content">
      <span class="paper-title">CT<sup>2</sup>: Colorization Transformer via Color Tokens</span>
      <span class="paper-author"><b>Shuchen Weng</b>*, Jimeng Sun*, Yu Li, Si Li, Boxin Shi</span>
      <span class="paper-venue">ECCV 2022 (Oral)</span>
      <div class="paper-links">
        <a href="https://ci.idm.pku.edu.cn/Weng_ECCV22b.pdf">[PDF]</a>
        <a href="https://github.com/shuchenweng/CT2">[Code]</a>
      </div>
    </div>
  </div>


  <h2>ğŸ’¼ Experience</h2>
  <ul>
    <li><b>Beijing Academy of Artificial Intelligence (BAAI)</b> | Technical Staff | <i>Jul. 2024 - Present</i></li>
    <li><b>Beijing Academy of Artificial Intelligence (BAAI)</b> | Research Intern | <i>Jan. 2023 - Jun. 2024</i></li>
    <li><b>Peng Cheng Lab</b> | Research Intern | <i>Feb. 2019 - Aug. 2019</i></li>
  </ul>

  <h2>ğŸ† Selected Awards</h2>
  <ul>
    <li>Huawei TopMinds Program (åä¸ºå¤©æ‰å°‘å¹´), 2024</li>
    <li>Alibaba Taotian Group T-star Program (é˜¿é‡Œæ·˜å¤©T-Star), 2024</li>
    <li>National Scholarship for Graduate Student, Peking University (Top 2%), 2023</li>
  </ul>

  <div style="margin-top: 80px; color: #888; font-size: 0.9em;">
    <p>Last updated: Jan. 2026</p>
  </div>

</body>
</html>
